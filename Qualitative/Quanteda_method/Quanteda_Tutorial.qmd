---
title: "Quanteda_Tutorial"
format: html
editor: visual
---

This document will follow a quanteda tutorial that I found online at this website: https://tutorials.quanteda.io/introduction/install/ 

#Packages
```{r}
#Packages that are needed to follow the tutorial given above

#install.packages("quanteda")
#install.packages("quanteda.textmodels")
#install.packages("quanteda.textstats")
#install.packages("quanteda.textplots")
#install.packages("readtext")
#devtools::install_github("quanteda/quanteda.corpora")
# install.packages("spacyr")
# install.packages("newsmap")
# install.packages("seededlda")

library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.sentiment)
library(readtext)
library(quanteda.corpora)
library(spacyr)
library(newsmap)
library(seededlda)
library(text2vec) #Semantic network analysis
library(lubridate)#needed for relative frequency analysis 
library(caret)
library(glmnet)
#Packages needed for my own coding style 
library(dplyr)
library(tidyverse)
library(ggplot2)

#packages needed for my built functions 
library(textclean)
#Function to adjust raw text 
replace_text<- function(text){
  
  #Separate number ranges for later conversion (3-4 to 3 to 4)
  # Replace hyphen-separated number ranges
  text <- gsub("(\\d+)-(\\d+)", "\\1 to \\2", text, perl = TRUE)
  #Replace colon-separated number ranges
  text <- gsub("(\\d+):(\\d+)", "\\1 to \\2", text, perl = TRUE)
  #Replace & with 'and' with a space before and after
  text <- gsub("([a-zA-Z])&([a-zA-Z])", "\\1 and \\2", text)
  
  #Replace numbers (1 to one)
  text<- replace_number(text, num.paste = T)
  #Replace ordinal (1st to first)
  text<- replace_ordinal(text, num.paste = T)
  
  return(text)
  
}
source("~/Professional/Github_repositories/Professional_code/Pro_code_project/Dummy_coding_data.R")

```

When loading data having a .csv file with a column containing the text is part of the tutorial
#Loading in data 
```{r}
interview_data<- read.csv("../../../../../Sample_data/KPMG_interview_responses.csv") %>%
  mutate(text = replace_text(text),
         date = as.Date(date, format = "%m/%d/%Y"),
         overall_experience_dummy = case_when(overall_experience == "positive"~1,
                                              overall_experience == "neutral"~0,
                                              overall_experience == "negative"~-1))
```

#Workflow: There are three basic types of objects in Quanteda. 

Corpus: 
saves character strings and variables in a dataframe. 
Combines texts with document-level variables

Tokens: 
Stores tokens in a list of vectors. 
More efficient than character strings, but preserves positions of words. 
Positional (string-of-words) analysis is performed using textstat_collocations(), tokens_ngrams() and tokens_select() or fcm() with window option.

Document-features Matrix (DFM): 
Represents frequencies of features in documents in a matrix 
The most efficient structure, but it does not have information on positions of words
Non-positional (bag-of-words) analysis are performed using many od the textstat_* and textmodel_* functions 


#Corpus

##Construct a corpus
We are using the "data frame" option since we are working with a .csv file. 
```{r}
#Constructing a corpus from the 'text' column in the interview data dataframe

corp_interview<- corpus(interview_data, text_field = 'text')
print(corp_interview)

summary(corp_interview, 5)

#Adding identifier to docnames
docid<- paste(interview_data$SID)
docnames(corp_interview)<- docid
print(corp_interview)
```


##Document-Level variables:
Quanteda's objects keep information associated with documents. they are called "document-level variables" or 'docvars' and are accessed using docvars
```{r}
head(docvars(corp_interview), 10)

#Extracting document-level variables

docvars(corp_interview, field = 'offer')
corp_interview$difficulty

#Assigning document-level variables
docvars(corp_interview, field = 'offer')<- ifelse(corp_interview$offer == 'accepted', 1,
                                                         ifelse(corp_interview$offer == 'no_offer' , 0, -1))

#Subsetting corpus 
corp_declined<- corpus_subset(corp_interview, offer == -1)
head(corp_declined)
ndoc(corp_declined)
##Different way of subsetting by a list 
corp_accepted<- corpus_subset(corp_interview, offer %in% c("accepted"))
head(corp_accepted)


```
##Change units of Texts.corpus_reshape() allows you to change the unit of texts between documents, paragraphs, and sentences. 
```{r}
corp_interview_sentences<- corpus_reshape(corp_interview, to = 'sentences')
head(corp_interview_sentences)
ndoc(corp_interview_sentences)

#Restore original
corp_doc<- corpus_reshape(corp_interview, to = 'documents')
ndoc(corp_doc)
```
##Extract tags from texts. Using corpus_segment() you can extract segments of texts and tags from documents. This is particularly useful when you analyze sections of documents or transcripts separately  
```{r}
#only works if input text file has tagged parts of the text
corp_tagged <- corpus(c("##INTRO This is the introduction.
                         ##DOC1 This is the first document.  Second sentence in Doc 1.
                         ##DOC3 Third document starts here.  End of third document.",
                        "##INTRO Document ##NUMBER Two starts before ##NUMBER Three."))
corp_sect <- corpus_segment(corp_tagged, pattern = "##*")

cbind(docvars(corp_sect), text = as.character(corp_sect))
print(corp_sect)

#Speaker Identifiers. Works if the transcripts have identifiers of who is talking when 
corp_speeches <- corpus("Mr. Smith: Text.
                        Mrs. Jones: More text.
                        Mr. Smith: I'm speaking, again.")
corp_speakers <- corpus_segment(corp_speeches, pattern = "\\b[A-Z].+\\s[A-Z][a-z]+:", valuetype = "regex")
cbind(docvars(corp_speakers), text = as.character(corp_speakers))

#Can use the corpus_segment function to separate sentences but it would be better and probably more accurate to use the corpus_reshape() functin that is described above 


```
#Tokens
Tokens segment texts in a corpus into tokens (words or sentences) by word boundaries 
##Constructing tokens 
```{r}
#Removing all objects excpet the data since it could get a little crowded in the environment 
rm(list = setdiff(ls(), "interview_data"))

#Corpus 
corpus_interview<- corpus(interview_data, text_field = "text")

#By default tokens() only removes separators (typically white spaces), but you can also remove punctuation and numbers 
tokens_interview<- tokens(corpus_interview)

#removes punctuations 
tokens_interview_punt<- tokens(corpus_interview, remove_punct = TRUE)
```
##Keywords-n-contexts
Allows us ot see how keywords are used in the actual contexts in a concordance view
```{r}
kw_interivew<- kwic(tokens_interview, pattern = 'KPMG')
head(kw_interivew)
#multiple examples
kw_interivew2<-kwic(tokens_interview, pattern = c("KPMG", "Interview"))
kw_interivew2

#window argument allows for amount of words to be displayed around the specified key word 
kw_interview3<- kwic(tokens_interview, pattern = c("KPMG*", "Interview*"), window = 7)
kw_interview3

#find multi-word expresisons, separate words by white space and wrap the character vector by phrase()
kw_interview4<- kwic(tokens_interview, pattern = phrase("Senior Associate*"))
kw_interview4
View(kw_interview4)
```
##Select Tokens
We can remove tokens that we are not interested in using. Most of the time it is 'stopwords' but can be customized for specific things 
```{r}
tokens_interview_nostop<- tokens_select(tokens_interview, pattern = stopwords('en'), selection = 'remove')
print(tokens_interview_nostop)

#Tokens_remove() is an alias to tokens_select(selection = 'remove') so the code below is equivalent
tokens_interview_nostop2<- tokens_remove(tokens_interview, pattern = stopwords('en'))
tokens_interview_nostop2

#Removing tokens changes the lengths of the documents, but they remind hte same if you set padding = true. 
tokens_interview_nostop3<- tokens_remove(tokens_interview, pattern = stopwords('en'), padding = T)
tokens_interview_nostop3

#Interestd in certain words
tokens_interview_associate<- tokens_select(tokens_interview, pattern = c("associate","KPMG"), padding =T)
tokens_interview_associate

#Interested in certain words with window 
tokens_interview_associate2<- tokens_select(tokens_interview, pattern = c("senior","KPMG"), padding = T, window = 10)
View(tokens_interview_associate2)
```
##Compound tokens
```{r}
#Can do various multi-word expressions 
kw_intetview_multi<- kwic(tokens_interview, pattern = phrase(c("Senior Associate*","KPMG*")), window = 10)
View(kw_intetview_multi)

#Preserve these expressions in a bag-of-word analysis, you have to compund thme using tokens_compound()
tokens_interview_compound<- tokens_compound(tokens_interview, pattern = phrase(c("Senior Associate*","KPMG*")))
tokens_interview_compound
kw_interview_compound<- kwic(tokens_interview_compound, pattern = c("Senior Associate*", "KPMG*"))
kw_interview_compound
```
##Look up dictionary
tokens_lookup() is the most flexible dictionary look up function in quanteda. Using dictionary() we can import dictionary files in the Wordstat, LIWC, Yoshicoder, Lexicoder and YAML formats. 
```{r}
#This is part of the quanteda package. It is the Lexicoder Sentiment Dictionary (LSD) 2015. It has a bunch of word patterns that are categorized into 'negative', 'positive', 'neg_positive' (word patterns indicating a positive word preceded by a negation "not good"), and 'neg_negative' (word patterns indicating a negative word preceded by a negation that convey and overall positive sentiment "not bad").
LSD_dict<- data_dictionary_LSD2015 

length(LSD_dict)
names(LSD_dict)

#Examples of neg_positive in the dictionary
LSD_dict[["neg_positive"]]
#Examples of neg_negative in the dictionary
LSD_dict[["neg_negative"]]

#The levels argument determines the keys to be recorded in a resulting tokens object 
token_interview_level1<- tokens_lookup(tokens_interview, dictionary = LSD_dict, levels = 1)
token_interview_level1

#We can explore the dictionary levels itself with 
dictionary(LSD_dict)
##Example with multiple levels since the LSD dict only has 4 but the newsmap has nested levels 
dictionary(data_dictionary_newsmap_en)

#We can run a keyword-in-context analysis by looking up mentions of specific levels of the selected dictionary
kwic(tokens_interview, LSD_dict[['negative']])

#We can also define our own dictionary 
manual_dict<- dictionary(list(negative = c("bad","hard","difficult"),
                              positive = c("good","easy", "not difficult")))
manual_dict

man_dict_tokens<- tokens_lookup(tokens_interview, dictionary = manual_dict)
View(man_dict_tokens)
```
##Generate N-Grams
N-grams are a sequence of tokens from already tokenized text objects
```{r}
#The length determines how many the tokens are segmented. So 2:4 shows how the tokens can be sequenced by groups of 2 and then groups of 3 and then groups of 4 and so on 
tokens_interview_ngram<- tokens_ngrams(tokens_interview_punt, n= 2:10)
tokens_interview_ngram[[1]]

#Tokens_ngrams() also supports skip to generate skip-grams. This produces the sequence but qwill skip over a range (provided by the skip argument) and will combine over both ranges. For example, the first few words of the first example text is "The interview process for" but with skip we see 'The_process' and 'The_for' so the word 'interview' is skipped and the n-gram has n=2 which is the sequence.
tokens_interview_skip<- tokens_ngrams(tokens_interview_punt, n = 2, skip = 1:2)
tokens_interview_skip

#Selective n-grams. we can use the tokens_compound to geenrate n-grams more selectively. For example we can make a negation bi-gram using phrase and a wild card regex *
tokens_interview_neg_bigram<- tokens_compound(tokens_interview, pattern = phrase("senior *"))
tokens_interview_neg_bigram_select<- tokens_select(tokens_interview_neg_bigram, pattern = phrase("senior_*"))
tokens_interview_neg_bigram_select
```

#Document Feature Matrix 
```{r}
#Removing all objects excpet the data since it could get a little crowded in the environment 
rm(list = setdiff(ls(), "interview_data"))
```
##Construct a DFM
```{r}
corp_interview<- corpus(interview_data, text_field = 'text', docid_field = "SID") #docid_field is how we retain the 'identifer' for each respondent
toks_interview<- tokens(corp_interview, remove_punct = T, remove_symbols = T,remove_numbers = T) 
dfmat_inter<- dfm(toks_interview)
dfmat_inter
#number of documents
ndoc(dfmat_inter)
#number of features 
nfeat(dfmat_inter)
#docnames 
docnames(dfmat_inter)
#View feature names 
featnames(dfmat_inter)

#Can use rowSeums and colSums to calculate marginals. Just shows how many features of 'tokens' per response
rowSums(dfmat_inter)
#top features or the most frequent features 
topfeatures(dfmat_inter, 10)

#Converting the frequency count ot a proportion within documents. The textstat_frequency() described in chapter 4 offers more advanced functionalities and returns a dataframe so that can be used later 
dfmat_inter_prop<- dfm_weight(dfmat_inter, scheme = 'prop')
dfmat_inter_prop

#Weight the frequency count by uniqueness of the features across documents 
dfmat_inter_unique<- dfm_tfidf(dfmat_inter)
dfmat_inter_unique
```
##Select features 
```{r}
#select feature from a DFM using dfm_select()
dfmat_inter_nostop<- dfm_select(dfmat_inter, pattern = stopwords('en'), selection = 'remove')
dfmat_inter_nostop

#also dfm_remove. This was in the 'tokens' stage as well but some analysis (topic modeling etc) might benefit more from removing stopwords at this stage to allow for more flexability

dfmat_inter_nostop2<- dfm_remove(dfmat_inter, pattern = stopwords('en'))
dfmat_inter_nostop2

#can also select features based on the length of features. 
dfmat_inter_long<- dfm_keep(dfmat_inter, min_nchar = 7) #Keeps features/tokens that are a minimal character length 
dfmat_inter_long

#combining with the top features
topfeatures(dfmat_inter_long, 10)

#dfm_trim selects features based on feature frequencies 
dfmat_inter_freq<- dfm_trim(dfmat_inter, min_termfreq = 10)
dfmat_inter_freq
#can use the dfm_trim for proportions 
dfmat_inter_docfreq<- dfm_trim(dfmat_inter, max_docfreq = .1, docfreq_type = 'prop')
dfmat_inter_docfreq
```
##Look up dictionary 
```{r}
#loading dictionary 
LSD_dict<- data_dictionary_LSD2015

dfmat_inter_sentiment<- dfm_lookup(dfmat_inter, dictionary = LSD_dict, levels = 1)
dfmat_inter_sentiment

#dfm_lookup cannot detect multi-word expressions since hte dfm does not store information about positions of words. They recommend using tokens_lookup() to detect or tokens-copound() to compound multi-word expressions before creating a DFM
```
##Group documents 
```{r}
#dfm_group() merges documents based on a vector given to the groups argument. In grouping documents it takes the sums of feature frequencies
#dfmat_inter <- dfm_remove(dfmat_inter, stopwords('en'))
dfmat_inter_exp<- dfm_group(dfmat_inter, groups = overall_experience)
dfmat_inter_exp
#dfm_group identifies document-level variables that are the same within gorups and keeps these variables 
docvars(dfmat_inter_exp)
```
#Feature co-occurence matrix 
This is different than a document frequency matrix (DFM). A feature co-occurence matrix (FCM) records the number of co-occurrences of tokens. This is a special objct in quanteda by behaves similarly to a DFM
```{r}
#Removing all objects excpet the data since it could get a little crowded in the environment 
rm(list = setdiff(ls(), "interview_data"))

```
##Construct a FCM 
This can be done from tokens or from a DFM 
```{r}
corpus_interview<- corpus(interview_data,docid_field = 'SID', text_field = 'text')
toks_interview<- tokens(corpus_interview, remove_punct = T, remove_symbols = T, remove_numbers = T) %>%
  tokens_select(., min_nchar = 2)
dfmat_inter<- dfm(toks_interview, tolower = T) %>%
  dfm_remove(., stopwords('en')) 

dfmat_inter<- dfm_trim(dfmat_inter, min_termfreq = 5)
dfmat_inter

nfeat(dfmat_inter)

fcmat_inter<-fcm(dfmat_inter)
fcmat_inter

topfeatures(fcmat_inter, n = 10)

#Can use FCM select and features 
feats<- names(topfeatures(fcmat_inter))
fcmat_inter_select<- fcm_select(fcmat_inter, pattern = feats, selection = 'keep')
fcmat_inter_select
```
##FCM semantic network analysis
A FCM can be used to train word embedding models with the 'text2vec' package or to visualize a semantic network analysis with textplot_network()
```{r}
size <- log(colSums(dfm_select(dfmat_inter, feats, selection = "keep")))

set.seed(144)
textplot_network(fcmat_inter_select, min_freq = 0.8, vertex_size = size / max(size) * 3)

experience_levels <- docvars(corpus_interview, "overall_experience")

# Iterate over each category of overall_experience
for (category in unique(experience_levels)) {
    # Filter the corpus and DFM for the current category
    subset_corpus <- corpus_interview[experience_levels == category]
    
    subset_toks<- tokens(subset_corpus, remove_punct = T, remove_symbols = T, remove_numbers = T)
    
    subset_dfm<- dfm(subset_toks, tolower = T) %>%
      dfm_remove(., stopwords('en')) %>%
      dfm_keep(., min_nchar = 2)
     
    #The top n features based off frequency. This can be adjusted to be based of co-occurrence 
    current_feats<- names(topfeatures(subset_dfm, n = 10))
    
    # Compute the Feature Co-occurrence Matrix (FCM) for the subset. Have to do the 'fcm_select' in order to narrow down the words displayed 
    fcm_subset <- fcm(subset_dfm) %>%
      fcm_select(., pattern = current_feats, selection = 'keep')
    
    # Compute sizes for the network
    size <- log(colSums(dfm_select(subset_dfm, pattern = current_feats, selection = "keep")))
    
    # Plot the semantic network for the subset
    set.seed(144)  # For reproducibility
    print(textplot_network(fcm_subset, min_freq = 0.5, vertex_size = size / max(size) * 3, omit_isolated = T)+
            ggtitle(paste0("Overall Experience", " ", category)))
  
}
```
#Statistical Analysis 
```{r}
#Removing all objects except the data since it could get a little crowded in the environment 
rm(list = setdiff(ls(), "interview_data"))
```

##Simple frequency analysis 
```{r}
corpus_inter<- corpus(interview_data, docid_field = 'SID', text_field = 'text')
toks_inter<- tokens(corpus_inter, remove_punct = T, remove_symbols = T, remove_numbers = T) 
dfmat_inter<- dfm(toks_inter, tolower = T) %>%
  dfm_remove(., stopwords('en')) %>%
  dfm_group(., groups = overall_experience)

#textstat_frequency shows both term and document frequencies and can use hte function to find the most frequenct features within groups 
tstat_freq<- textstat_frequency(dfmat_inter, n = 5, groups = overall_experience)
tstat_freq

#plotting version on ggplot with counts
dfmat_inter %>%
  textstat_frequency(n = 10, groups = overall_experience) %>%
  ggplot(aes(x = reorder(feature, frequency), y = frequency, color = group))+
  geom_point()+
  coord_flip()+
  labs(x = NULL, y = "frequency")+
  theme_minimal()

#word cloud 
set.seed(123)
textplot_wordcloud(dfmat_inter, max_words = 30)

#wordcloud by groups
textplot_wordcloud(dfmat_inter, comparison = T, max_words = 30)

```
##Lexical diversity 
textstat_lexdiv() calculates various lexical diversity measures based on the number of unique types of tokens and lengh of document. 
```{r}

tstat_lexdiv<- textstat_lexdiv(dfmat_inter)
tstat_lexdiv

#Plot of the lexical diversity 
plot(tstat_lexdiv$TTR, type = "l", xaxt = "n", xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(tstat_lexdiv)), labels = dfmat_inter$overall_experience)
```
##Document/feature similarity 
textstat_dist90 caluclates similarites of documents or features for various measures. THe output is compatible with Rs dist() so hierarchical clustering cna be performed without any transformation
```{r}
dfmat_inter<- dfm(toks_inter, tolower = T) %>%
  dfm_remove(., stopwords('en')) %>%
  dfm_group(., groups = docname_)

tstat_dist<- as.dist(textstat_dist(dfmat_inter))
clust<- hclust(tstat_dist)

plot(clust, xlab = "Distance", ylab = NULL)
```
##Relative frequency analysis (keyness)
using textstat_keyness() you cna compare frequencies of words between target and reference documents. In a tutorial the target documents are news articles published in 2016 and reference documents are those published in 2012-2015. In our example it will be the date of the interview review. The lubridate package to retrieve the year of review

Keyness is signed two-by-two association score originally implemented in WordSmith to identify frequent words in documents in a target and reference group.

The target group refers to the set of documents or texts that we are interested in analyzing while the reference group serves as a comparison set. 
The goal is to identify words that are significantly more frequent in the target group compared to the reference group. 

Chi-square metric: common statisitc that is used in the keyness analysis to measure association between word frequences and the groups being compared 
it quanitifes how much the observed frequencies of words in the target group deviate from what would be expceted if there were no association between the words and the groups. 
Higher chi-square values indicate stronger assocaitions between words and the target group. 


```{r}
#Redoing the corpus and tokens stage to remove the groups of 'overall_experience' that was created in the chunks above 
corpus_inter<- corpus(interview_data, docid_field = 'SID', text_field = 'text')
toks_inter<- tokens(corpus_inter, remove_punct = T, remove_symbols = T, remove_numbers = T)

dfmat_inter<- dfm(toks_inter, tolower = T) %>%
  dfm_remove(., stopwords('en')) %>%
  dfm_keep(., min_nchar = 7) 

tstat_key<- textstat_keyness(dfmat_inter,
                             target = year(dfmat_inter$date) >= 2022)
#For some reason we cannnot do all of the documents. Investigate this further.

#plotting 
textplot_keyness(tstat_key)

#In our example what this means is that words with higher chi-square value have a stronger association for interview reviews that take place after 2022. This could be better with phrases and could help with identifying changing trends in the data as time goes on. So specifically the word 'applied' has a better association to beyond the year 2022 compared to 2021 and previous years. Could be that a hiring freeze or just less applicants took place that year 

```
##Collocation analysis 
A collocation analysis allows us to identify contriguous collocation of words. One of the most common types of multi-word expressions are proper names which cna identified simply based on capialitzation in english texts. This is the tutorials demonstrastion and they are using the tokens_select function to manually look for specific things 

This is different than a dictionary look-up because it uses statistical measures (log-liklihood ratio) for combinations of words and does not rely on predefiend lists of expressions but rather identifies significant patterns based on their frequency and co-occurrence within the corpus 
```{r}
#Redoing the corpus and tokens stage to remove the groups of 'overall_experience' that was created in the chunks above 
corpus_inter<- corpus(interview_data, docid_field = 'SID', text_field = 'text')
toks_inter<- tokens(corpus_inter, remove_punct = T, remove_symbols = T, remove_numbers = T)

dfmat_inter<- dfm(toks_inter, tolower = T) %>%
  dfm_remove(., stopwords('en'))

dmat_top_feats<- topfeatures(dfmat_inter, n = 20)

# Select tokens beginning with "interviewer" (case-sensitive) and generate collocations
tstat_col_interviewer <- tokens_select(toks_inter, 
                                       pattern = "^interview",  # Adjust regex pattern
                                       valuetype = "regex", 
                                       case_insensitive = T, 
                                       padding = TRUE) %>% 
                        textstat_collocations(min_count = 1)

# Display the collocations
print(tstat_col_interviewer)

```
#Advanced Operations
```{r}
#Removing all objects except the data since it could get a little crowded in the environment 
rm(list = setdiff(ls(), "interview_data"))

```

##Compute similarity between authors 
we can compute the similarities between authors by grouping their documents and comparing them with all other authors. In the tutorial they group twitter posts by handle names and similarities between the users. we are going to try and find the similarities and dissimilarities between 'overall_experience'
```{r}
corpus_inter<- corpus(interview_data, docid_field = 'SID', text_field = 'text')
toks_inter<- tokens(corpus_inter, remove_punct = T, remove_symbols = T, remove_numbers = T) 
dfmat_inter<- dfm(toks_inter, tolower = T) %>%
  dfm_remove(., stopwords('en')) %>%
  dfm_group(., groups = overall_experience)

ndoc(dfmat_inter)

#Remove rare (less than 3 times) and short (one character) features and keep reviews that are 10 tokens or more. 
dfmat_exp<- dfmat_inter %>%
  dfm_select(min_nchar = 2) %>%
  dfm_trim(min_termfreq = 3) 

dfmat_exp<- dfmat_exp[ntoken(dfmat_exp) > 10,]

#claculate overall experience similarity using textstat_dist()

tstat_dist<- as.dist(textstat_dist(dfmat_exp))
exp_clust<- hclust(tstat_dist)
plot(exp_clust)

#This was very similar to the 'Document/feature similarity' chunk above
```
##Compound multi-word expressions
We can compound multi-word expression through collocation analysis. In the tutorial example they clarify sequences of capitalized words and compound them as proper names. For our purpose 
```{r}
toks_interviewer<- tokens_select(toks_inter, 
                                pattern = "\\b(HR|manager|partner|panel|interviewer|recruiter)\\b",
                                valuetype = 'regex',
                                case_insensitive = F,
                                padding = T)

tstat_interviewer<- textstat_collocations(toks_interviewer, min_count = 1)
tstat_interviewer

#Compounding only stronger associated multi-word expressions by subsetting the tstat_interviewere object where z>3

toks_comp<- tokens_compound(toks_interviewer, pattern = tstat_interviewer[tstat_interviewer$z >3,],
                            case_insensitive = F)
kw_comp<- kwic(toks_comp, pattern = "HR_*")
kw_comp
```
##Apply Dictionary to specific contexts 
Wew can detect occurrences of words in specific contexts by selectively applying dictionary. In the tutorial example they applie a sentiment dictionary to segments of news articles that mentions the "British Government" 

For our example i am applying a sentiment dictionary to text that mentions an interviewer
```{r}
interviewer<- c("interviewer", "HR",'hr','partner', 'manager', 'panel', 'Recruiter')
#Tokenize texts and select tokens surrounding keywords related to the interviewer using the 'tokens_keep' function
toks_interviewer<- tokens_keep(toks_inter, pattern = phrase(interviewer), window = 10)

#Apply the lexicoder sentiment dictionary to the selected context 
lengths(data_dictionary_LSD2015)

#Select only the negative and positive sentiment categories from the dictionary 
data_dictionary_LSD2015_pos_neg<- data_dictionary_LSD2015[1:2]

toks_interviewer_pos_neg<- tokens_lookup(toks_interviewer, dictionary = data_dictionary_LSD2015_pos_neg)

#Overall experience compared to sentiment words 
dfmat_interviewer<- dfm(toks_interviewer_pos_neg) %>%
  dfm_group(groups = overall_experience)

#plotting 
matplot(as.factor(dfmat_interviewer$overall_experience), dfmat_interviewer, type = 'l', lty = 1, col = 1:2, ylab="Frequency", xlab = "")
grid()
legend('topleft', col = 1:2, legend = colnames(dfmat_interviewer), lty = 1, bg = 'white')


#Date compared to sentiment words 
dfmat_interviewer<- dfm(toks_interviewer_pos_neg) %>%
  dfm_group(groups = date)

#plotting 
matplot(dfmat_interviewer$date, dfmat_interviewer, type = 'l', lty = 1, col = 1:2, ylab="Frequency", xlab = "")
grid()
legend('topleft', col = 1:2, legend = colnames(dfmat_interviewer), lty = 1, bg = 'white')

#Offer extended to sentiemnt words 
dfmat_interviewer<- dfm(toks_interviewer_pos_neg) %>%
  dfm_group(groups = offer)

#plotting 
matplot(as.factor(dfmat_interviewer$offer), dfmat_interviewer, type = 'l', lty = 1, col = 1:2, ylab="Frequency", xlab = "")
grid()
legend('topright', col = 1:2, legend = colnames(dfmat_interviewer), lty = 1, bg = 'white')


#Can compute the sentiment scores by taking the differnce between the fequency of positive and negative words over the years
dfmat_interviewer<- dfm(toks_interviewer_pos_neg) %>%
  dfm_group(groups = date)

plot(dfmat_interviewer$date, 
     dfmat_interviewer[, 'positive'] - dfmat_interviewer[, 'negative'],
     type = 'l', ylab= 'Sentiment Scores', xlab = "")
grid()
abline(h = 0, lty = 2)

#Apply a kernel smoothing ot show the trend more clearly 
dat_smooth<- ksmooth(dfmat_interviewer$date,
                     y = dfmat_interviewer[, 'positive'] - dfmat_interviewer[, 'negative'],
                     kernel = 'normal',
                     bandwidth = 30)
plot(dat_smooth$x, dat_smooth$y, type = 'l', ylab = "Sentiment Scores", xlab = "")
grid()
abline(h =0, lty = 2)
```
##Identify related words of keywords
we can identify related words of keywords based on their distance in the documents. In the tutorial they created a list of words related to the european union by comparing frequency of words inside and outside of their contexts 
```{r}
#Selecting two tokens objects for words inside and outside of the 10-word windows of the keywords (keys)
keys<- c("interviewer", "manager", "hr","HR","partner","recruiter", "director")
toks_inside<- tokens_keep(toks_inter, pattern = keys, window = 10)
toks_inside<- tokens_remove(toks_inside, patter = keys) #removing hte keywords
toks_outside<- tokens_remove(toks_inter, pattern = keys, window =10)

#Compute words' association with the keywords using textstat_keyness()
dfmat_inside <- dfm(toks_inside) %>%
  dfm_remove(., stopwords('en'))
dfmat_outside <- dfm(toks_outside) %>%
  dfm_remove(., stopwords('en'))

tstat_key_inside <- textstat_keyness(rbind(dfmat_inside, dfmat_outside), 
                                     target = seq_len(ndoc(dfmat_inside)))
head(tstat_key_inside, 50)

#Explanation of output:The keyness metric (often based on the chi-square statistic or other statistical measures) helps identify words or features that occur significantly more frequently within the specified context compared to outside of it. In this case, "interview" is deemed to be highly associated with the context defined by your keywords, suggesting that it frequently occurs within the proximity of these terms in your text corpus.


```
#Text Scaling and Classification
How to derive latent positions from text data and how to classify documents 
```{r}
#Removing all objects except the data since it could get a little crowded in the environment 
rm(list = setdiff(ls(), "interview_data"))
```
##Naive Bayes Classifier
Naive Bayes is a supervised model usually used to classify documents into two or more categories. We train the classifier using labels attached to documents, and predict the most likely class(es) of new unlabeled documents 
```{r}
#In the tutorial they are using 2000 movie reviews and they have the movie reviews classified as 'positive' or 'negative' in the 'sentiment' column we have something similar in the 'overall_experience' column but we only have 70 reviews. The process will go with 52 Interviews to be trained and 18 to be tested. 

#Create corpus
corpus_inter<- corpus(interview_data)

set.seed(123)
id_train<- sample(1:nrow(interview_data), (70 * .8), replace = F)

#Create docvar with ID 
corpus_inter$id_numeric<- 1:ndoc(corpus_inter)

#tokenizing 
toks_inter<- tokens(corpus_inter, remove_punct = T, remove_numbers = T )%>%
  tokens_remove(pattern = stopwords('en'))

dfmat_inter<- dfm(toks_inter) %>%
  dfm_keep(.,  min_nchar = 2)

#get training set 
dfmat_inter_train<- dfm_subset(dfmat_inter, id_numeric %in% id_train)

#Get testing set 
dfmat_inter_test<- dfm_subset(dfmat_inter, !id_numeric %in% id_train)

#Training the NB classifier 
tmod_nb<- textmodel_nb(x = dfmat_inter_train, y = dfmat_inter_train$overall_experience, prior = 'docfreq', distribution = 'multinomial')
summary(tmod_nb)

#The coeficients represent how close the features are to being associated with the intended class. So a higher coeficient would mean a stronger association between that feature and class

#Naive Bayes can only take features into consideration that occur both in the training set and the test set, but we can make the features identical using dfm_match()

dfmat_matched<- dfm_match(dfmat_inter_test, features = featnames(dfmat_inter_train))

#See how well the classification worked. 
actual_class<- dfmat_matched$overall_experience
predicted_class<- predict(tmod_nb, newdata = dfmat_matched)
tab_class<- table(actual_class, predicted_class)
tab_class

#A good bit of incorrect classifciation but the n is very low so that is predicted. Confusion matrix to give overall model accuracy
confusionMatrix(tab_class, mode = "everything", positive = "positive")

#Precision, recall and the F1 score are frequently used to assess the classification performance. 
#Precision is measured as TP / (TP + FP), where TP are the number of true positives and FP are the false positives. 
#Recall divides the true positives by the sum of true positives and false negatives TP / (TP + FN). 
#Finally, the F1 score is a harmonic mean of precision and recall 2 * (Precision * Recall) / (Precision + Recall).

#This could be useful because if there is a good enough 


```
##Regularized Regression Classifier. Did not work with my example since the data was too small but it worked with the tutorial examples
Regularized regression is a classification technique where the categroy of interest is regressed on text features using a penalized form of regression where parameter estimates are biased towards 0. Here we will be using a specifc type of regularized regression, the Least Absolute Shrinkage and Selection Operator (LASSO) simialr to the ridge regression. 

In the LASSO the degree of penalization is determined by a regularization parameter lambda. We can use the cross-validation function available in the blmnet pacakge to select the optimal value ofr lambda. We train the classifier using lass labels attached ot documents and predict the most liekly class(ses) of new unlabelled document.s Althought regularized regression is not part of the quanteda.textmodels package, the functions for regularized regression from the the glmnet package can be easily worked into a qunteda workflow. 

The tutorial online used the same data for the naive bayesian walkthrough so we are going to do the same and save some time 
```{r}
#Here are the training and testing sets from the naive bayesian walkthrough
dfmat_inter_train
dfmat_inter_test

#Next we choose lamda using cv.glmnet from the glmnet package. cv.glmnet requires an input matrix x and a response vector y. For the input matrix we will use the training set converted to a psare matrix. For the response vector we will use a trichotmous indicator of review snetiment in the trainin set with positive reviews coded as 1 and negative reviews coded as -1 and neutral coded as 0 

lasso<- cv.glmnet(x = dfmat_inter_train,
                  y = dfmat_inter_train$overall_experience_dummy,
                  alpha = 1,
                  nfold = 5, 
                  family = "multinomial")
#As an initial evaluation of the model we can print the most predictive features. We begin by obtaining the best value of lambda 
index_best <- which(lasso$lambda == lasso$lambda.min)
beta <- lasso$glmnet.fit$beta[index_best]


```

##Wordscoresf(typically used for positions (mostly for political actors). Given that I am going to move onto someting else 
```{r}

```

##Wordfish
Wordfish is a Poisson scaling model of one-dimensional document positions. Wordifsh also allows for scaling documents but in comparison to wordscores, reference scores/ texts are not required. Wordfish is an unsupervised one-dimensional text scaling method, meaning that it estimates the positions of documents solely based on the observed word frequencies

I don't really understand this all that much 
```{r}
tmod_wf<- textmodel_wordfish(dfmat_inter, dir = c(6,5))
summary(tmod_wf)
textplot_scale1d(tmod_wf)
#Textplot with groups 
textplot_scale1d(tmod_wf, groups = dfmat_inter$overall_experience)

#Textplot of specific words
textplot_scale1d(tmod_wf, margin = "features", 
                 highlighted = c("interview", "hr","manager","recruiter","partner", "round"))
```
##Correspondence analysis 
Correspondence analysis is a technique to scale documents on multiple dimensions. Correspondence analysis is similar to principal component analysis but works for categorical variables (contingency table).
```{r}
tmod_ca<- textmodel_ca(dfmat_inter)

#plotting
textplot_scale1d(tmod_ca)


#plot documents on multi-dimensional scale
dat_ca<- data.frame(dim1 = coef(tmod_ca, doc_dim = 1)$coef_document,
                    dim2 = coef(tmod_ca, doc_dim = 2)$coef_document)
dat_ca

#Plotting multiple dimensions
plot(1, xlim = c(-2, 2), ylim = c(-2, 2), type = "n", xlab = "Dimension 1", ylab = "Dimension 2")
grid()
text(dat_ca$dim1, dat_ca$dim2, labels = rownames(dat_ca), cex = 0.8, col = rgb(0, 0, 0, 0.7))
```

##Topic modeling 
topics models are unsupervised document classification techniques. By modeling distributions of topics over words and words over documents, topic models identify the most discriminatory groups of documents automatically. 
```{r}
#Token creation and document frequency matrix with cleaning 
toks_inter<- tokens(corpus_inter, remove_punct = T, remove_numbers = T, remove_symbols = T)%>%
  tokens_remove(pattern = stopwords('en'))

#After removing stopwords, numbers, punctuation, symbols we wil lonly keep the top 20% of the most frequent features (min_termfreq = .8) that appear in less than 10% of all documents (max_docfreq = .1) using dfm_trim() to focus on common but distinguishing features 
dfmat_inter<- dfm(toks_inter) %>%
  dfm_keep(.,  min_nchar = 2) %>%
  dfm_trim(min_termfreq = .8, termfreq_type = 'quantile',
           max_docfreq = .1, docfreq_type = 'prop')
```
### Latent Dirichlet Allocation (LDA). Quanteda does not implement but fits it using other packages 
```{r}
#K = 5 specifies the number of topcis to be discovered this is an important parameters that needs to be played with and validated 
tmod_lda<- textmodel_lda(dfmat_inter, k = 5)
terms(tmod_lda,10)

#We can then obtain the most likely toics using topics() and save them as a document-level variable
head(topics(tmod_lda), 20)

# assign topic as a new document-level variable
dfmat_inter$topic <- topics(tmod_lda)

# cross-table of the topic frequency
table(dfmat_inter$topic)
```
###Seeded LDA
In the seeded LDA we can pre-define topics in LDA using a dictionary of seed words. THe key here is building the dictionary to be used for future use.
```{r}
#The number of topics is determined by the number of keys in the dictionary. Next, we can fit the seeded LDA model using textmodel_seededlda() and specify the dictionary with our relevant keywords.
LSD_dict<- data_dictionary_LSD2015[1:2]
tmod_slda<- textmodel_seededlda(dfmat_inter, dictionary = LSD_dict)

terms(tmod_slda,20)



```

